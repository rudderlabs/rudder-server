# Contains the configuration for the whole server
# Dont set secrets, passwords, api_keys etc. in this file. Use '.env' file or environment variables for that

# These values can be overriden through environment variables. The regex for these is present in config/config.go
# Please look at the following examples to override the config
#
# Eg 1:
# maxProcess = 12
#
# RSERVER_MAX_PROCESS=12
#
# Eg 2:
# [RateLimit]
# eventLimit=1000
#
# RSERVER_RATE_LIMIT_EVENT_LIMIT=1000
#
# Eg 3:
# [Gateway]
# maxDBWriterProcess = 256
#
# RSERVER_GATEWAY_MAX_DBWRITER_PROCESS=64
#
# Eg 4:
# [BackendConfig]
# configJSONPath = "./workspaceConfig.json"
#
# RSERVER_BACKEND_CONFIG_CONFIG_JSONPATH="./workspaceConfig.json"
#
# Eg 5:
# [JobsDB.backup]
# enabled = true
# RSERVER_JOBS_DB_BACKUP_ENABLED=true
#
# Eg 6:
# [Processor]
# loopSleepInMS = 10
# RSERVER_PROCESSOR_LOOP_SLEEP_IN_MS=10
#
# Eg 7:
# [Router]
# readSleepInMS = 1000
# RSERVER_ROUTER_READ_SLEEP_IN_MS=1000



maxProcess = 12 # Max number of cpu process to be used by go runtime
gwDBRetentionInHr = 0
routerDBRetention = 0
enableProcessor = true
enableRouter = true
enableStats = true

[Http]
ReadTimeOutInSec = 0
ReadHeaderTimeoutInSec = 0
WriteTimeOutInSec = 10
IdleTimeoutInSec = 720
MaxHeaderBytes = 524288

[RateLimit]
eventLimit=1000
rateLimitWindowInMins=60
noOfBucketsInWindow=12

[Gateway]
webPort = 8080
maxUserWebRequestWorkerProcess = 64
maxDBWriterProcess = 256
CustomVal = "GW"
maxUserRequestBatchSize = 128
maxDBBatchSize = 128
userWebRequestBatchTimeoutInMS = 15
dbBatchWriteTimeoutInMS = 5
maxReqSizeInKB = 4000
enableRateLimit = false
enableSuppressUserFeature = true
allowPartialWriteWithErrors = true
allowReqsWithoutUserIDAndAnonymousID = false

[EventSchemas]
enableEventSchemasFeature = false
syncIntervalInS = 5
noOfWorkers = 128

[Gateway.webhook]
batchTimeoutInMS = 20
maxBatchSize = 32
maxTransformerProcess = 64
maxRetry = 5
maxRetryTimeInS = 10

[SourceDebugger]
maxBatchSize = 32
maxESQueueSize = 1024
maxRetry = 3
batchTimeoutInS = 2
retrySleepInMS = 100
disableEventUploads = false

[DestinationDebugger]
maxBatchSize = 32
maxESQueueSize = 1024
maxRetry = 3
batchTimeoutInS = 2
retrySleepInMS = 100
disableEventDeliveryStatusUploads = false

[Archiver]
backupRowsBatchSize = 100

[JobsDB]
# Migration related parameters
jobDoneMigrateThres = 0.8
jobStatusMigrateThres = 5
maxDSSize = 100000
maxMigrateOnce = 10
maxMigrateDSProbe = 10
maxTableSizeInMB = 300
migrateDSLoopSleepDurationInS = 30
addNewDSLoopSleepDurationInS = 5
refreshDSListLoopSleepDurationInS = 5
backupCheckSleepDurationIns = 5
backupRowsBatchSize = 1000
archivalTimeInDays = 10
archiverTickerTimeInMin = 1440

[JobsDB.backup]
enabled = true

[JobsDB.backup.gw]
enabled=true
pathPrefix=""

[JobsDB.backup.rt]
enabled=true
failedOnly=true

[JobsDB.backup.batch_rt]
enabled=false
failedOnly=false

[Router]
jobQueryBatchSize = 10000
updateStatusBatchSize = 1000
readSleepInMS = 1000
fixedLoopSleepInMS = 0
noOfJobsPerChannel = 1000
noOfJobsToBatchInAWorker = 20
jobsBatchTimeoutInSec = 5
maxSleepInS = 60
minSleepInS = 0
maxStatusUpdateWaitInS = 5
useTestSink = false
guaranteeUserEventOrder = true
kafkaWriteTimeoutInSec = 2
kafkaDialTimeoutInSec = 10
minRetryBackoffInS = 10
maxRetryBackoffInS = 300
# Below config variables are supported per destination type too
noOfWorkers = 64
allowAbortedUserJobsCountForProcessing=1
maxFailedCountForJob = 3
retryTimeWindowInMins = 180
    [Router.MARKETO]
    noOfWorkers = 8
    [Router.throttler.MARKETO]
    limit = 45
    timeWindowInS = 20
    [Router.BRAZE]
    forceHTTP1=true
    httpTimeoutInS=120
    httpMaxIdleConnsPerHost=32
    # settings similar to below one can be used to throttle router
    # sending events to downstream destinations
    # NOTE: Its a good practice to set timeWindowInS as high as possible. This will prevent blowup of status table.
    # [Router.throttler.AM]
    # limit = 1000
    # timeWindowInS =          1
    # userLevelThrottling =    true
    # userLevelLimit =         10
    # userLevelTimeWindowInS = 1

[BatchRouter]
mainLoopSleepInS = 2
jobQueryBatchSize = 100000
uploadFreqInS = 30
warehouseServiceMaxRetryTimeinHr = 3
# Below config variables are supported per destination type too
noOfWorkers = 8
maxFailedCountForJob = 128
retryTimeWindowInMins = 180

[Warehouse]
mode = "embedded" # modes - embedded, off, master, slave, master_and_slave
webPort = 8082
uploadFreqInS = 1800
noOfWorkers = 8
noOfSlaveWorkerRoutines = 4
mainLoopSleepInS = 5
minRetryAttempts = 3
retryTimeWindowInMins = 180
minUploadBackoffInS = 60
maxUploadBackoffInS = 1800
warehouseSyncPreFetchCount = 10
warehouseSyncFreqIgnore = false
stagingFilesBatchSize = 960
enableIDResolution = false
populateHistoricIdentities = false
    [Warehouse.redshift]
    maxParallelLoads = 3
    setVarCharMax = false
    [Warehouse.snowflake]
    maxParallelLoads = 3
    [Warehouse.bigquery]
    maxParallelLoads = 20
    [Warehouse.postgres]
    maxParallelLoads = 3
    [Warehouse.clickhouse]
    maxParallelLoads = 3
    queryDebugLogs = false
    blockSize = 1000
    poolSize = 10
    disableNullable = false

[Processor]
webPort = 8086
loopSleepInMS = 10
maxLoopSleepInMS = 5000
fixedLoopSleepInMS = 0
maxLoopProcessEvents = 10000
avgEventsInRequest = 1
transformBatchSize = 100
userTransformBatchSize = 200
sessionThresholdEvents = 100
sessionInactivityThresholdInS = 120
maxChanSize = 2048
processSessions = false
numTransformWorker = 8
maxRetry = 30
retrySleepInMS = 100
errReadLoopSleepInS = 30
errDBReadBatchSize = 10000
noOfErrStashWorkers = 2
maxFailedCountForErrJob = 3

[Dedup]
enableDedup = false
dedupWindowInS = 86400

[BackendConfig]
configFromFile = false
configJSONPath = "/etc/rudderstack/workspaceConfig.json"
pollIntervalInS = 5
regulationsPollIntervalInS = 300
maxRegulationsPerRequest = 1000


# If the server crashes 'crashThreshold' times in 'durationInS' seconds,
#       then we mark it degraded
# If the server crashes 'degradedCrashThreshold' times in 'degradedDurationInS' seconds in degraded mode,
#       then we stop the server
# Stores related metadata in storagePath
[recovery]
enabled = true
errorStorePath = "/tmp/error_store.json"
storagePath = "/tmp/recovery_data.json"
    [recovery.normal]
    crashThreshold = 5
    durationInS = 300

# logger configuration
[Logger]
enableConsole=true
enableFile=false
consoleJsonFormat=false
fileJsonFormat=false
logFileLocation="/tmp/rudder_log.log"
logFileSize = 100 # in MB
enableTimestamp=true
enableFileNameInLog=true
enableStackTrace=false
#moduleLevels="processor.transformer=ERROR:batchrouter=ERROR:pgnotifier=ERROR:warehouse=ERROR:enterprise.migrator=ERROR"

[Diagnostics]
enableDiagnostics = true
gatewayTimePeriodInS = 60
routerTimePeriodInS = 60
batchRouterTimePeriodInS = 600
enableServerStartMetric = true
enableConfigIdentifyMetric = true
enableServerStartedMetric = true
enableConfigProcessedMetric = true
enableGatewayMetric = true
enableRouterMetric = true
enableBatchRouterMetric = true
enableDestinationFailuresMetric = true


# runtime stats are collected using go runtime package and published to statsd
[RuntimeStats]
enabled=true
statsCollectionInterval=10
enableCPUStats=true
enableMemStats=true
enableGCStats=true

[PgNotifier]
retriggerIntervalInS = 2
retriggerCount = 500
trackBatchIntervalInS = 2
maxAttempt = 3
